{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\pc\\anaconda3\\anaconda\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\pc\\anaconda3\\anaconda\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset as numpy arrays\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.executor.memory =  None\n",
      "spark.driver.memory =  None\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"spark-dl-inference\") \\\n",
    ".master(\"local[*]\") \\\n",
    ".config(\"spark.executor.memory\", \"8g\") \\\n",
    ".config(\"spark.driver.memory\", \"8g\") \\\n",
    ".config(\"spark.python.worker.reuse\",True) \\\n",
    ".getOrCreate()\n",
    "# Create a SparkConf object\n",
    "conf=SparkConf()\n",
    "\n",
    "print(\"spark.executor.memory = \", conf.get(\"spark.executor.memory\"))\n",
    "print(\"spark.driver.memory = \", conf.get(\"spark.driver.memory\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 28, 28), (70000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate([train_images, test_images])\n",
    "y = np.concatenate([train_labels, test_labels])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "DATASET_SIZE = 70000\n",
    "TRAIN_RATIO = 0.9\n",
    "TEST_RATIO = 0.1\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(X, y, test_size=(1-TRAIN_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize\n",
    "train_images_1D = train_images.reshape(-1, 784) / 255.0\n",
    "test_images_1D = test_images.reshape(-1, 784) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63000, 784), (7000, 784))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_1D.shape, test_images_1D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63000, 28, 28, 1), (7000, 28, 28, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], 1)\n",
    "test_images = test_images /255.0\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407050 (1.55 MB)\n",
      "Trainable params: 407050 (1.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a simple FCN model\n",
    "def create_dense_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "dense_model = create_dense_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               401536    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484714 (1.85 MB)\n",
      "Trainable params: 484714 (1.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "# Define a simple CNN model\n",
    "def create_CNN_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "CNN_model = create_CNN_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1969/1969 [==============================] - 11s 5ms/step - loss: 0.2135 - sparse_categorical_accuracy: 0.9371 - val_loss: 0.1292 - val_sparse_categorical_accuracy: 0.9574\n",
      "Epoch 2/5\n",
      "1969/1969 [==============================] - 13s 6ms/step - loss: 0.0928 - sparse_categorical_accuracy: 0.9711 - val_loss: 0.0982 - val_sparse_categorical_accuracy: 0.9696\n",
      "Epoch 3/5\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.0949 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 4/5\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.0847 - val_sparse_categorical_accuracy: 0.9739\n",
      "Epoch 5/5\n",
      "1969/1969 [==============================] - 12s 6ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.0833 - val_sparse_categorical_accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28b923c7490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.fit(train_images_1D,\n",
    "          train_labels,\n",
    "          epochs=5,\n",
    "          validation_data=(test_images_1D, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 226ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-13.5157175,  -3.070777 ,  13.363666 ,  -4.9265337,  -6.419355 ,\n",
       "        -10.36186  ,  -4.50889  ,  -5.360558 ,  -1.210833 , -13.618066 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_1D = test_images_1D[:1]\n",
    "prediction = dense_model.predict(test_img_1D)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Prediction: {}\".format(np.argmax(prediction)))\n",
    "plt.imshow(test_img_1D.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.one_hot(train_labels.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(test_labels.astype(np.int32), depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([63000, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63000, 28, 28, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 28, 28, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1969/1969 [==============================] - 69s 34ms/step - loss: 0.1666 - acc: 0.9502 - val_loss: 0.0651 - val_acc: 0.9810\n",
      "Epoch 2/5\n",
      "1969/1969 [==============================] - 71s 36ms/step - loss: 0.0698 - acc: 0.9804 - val_loss: 0.0433 - val_acc: 0.9867\n",
      "Epoch 3/5\n",
      "1969/1969 [==============================] - 80s 41ms/step - loss: 0.0608 - acc: 0.9834 - val_loss: 0.0498 - val_acc: 0.9881\n",
      "Epoch 4/5\n",
      "1969/1969 [==============================] - 81s 41ms/step - loss: 0.0579 - acc: 0.9843 - val_loss: 0.0411 - val_acc: 0.9879\n",
      "Epoch 5/5\n",
      "1969/1969 [==============================] - 80s 41ms/step - loss: 0.0591 - acc: 0.9849 - val_loss: 0.0414 - val_acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28b99d3b110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.fit(train_images,\n",
    "          y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(test_images, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0414 - acc: 0.9891\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = CNN_model.evaluate(test_images, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = os.path.join(os. getcwd(), \"mnist_dense_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_dense_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_dense_model\\assets\n"
     ]
    }
   ],
   "source": [
    "dense_model.save('mnist_dense_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model_dir = os.path.join(os. getcwd(), \"mnist_CNN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_CNN_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_CNN_model\\assets\n"
     ]
    }
   ],
   "source": [
    "CNN_model.save('mnist_CNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407050 (1.55 MB)\n",
      "Trainable params: 407050 (1.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('mnist_dense_model')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 784) dtype=float32 (created by layer 'dense_input')>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               401536    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484714 (1.85 MB)\n",
      "Trainable params: 484714 (1.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_CNN_model = tf.keras.models.load_model('mnist_CNN_model')\n",
    "new_CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-13.5157175,  -3.070777 ,  13.363666 ,  -4.9265337,  -6.419355 ,\n",
       "        -10.36186  ,  -4.50889  ,  -5.360558 ,  -1.210833 , -13.618066 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(test_images_1D[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_images_1D[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 191ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.6484592e-14, 3.0058970e-12, 1.0000000e+00, 4.6788985e-14,\n",
       "        8.7341125e-15, 4.5458533e-19, 5.7536149e-15, 8.6449589e-12,\n",
       "        1.7475658e-11, 1.0818055e-17]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.shape\n",
    "example = test_images[:1]\n",
    "new_CNN_model.predict(test_images[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 784)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array to pandas DataFrame\n",
    "test_pdf = pd.DataFrame(test_images_1D)\n",
    "test_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.8 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.createDataFrame(test_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 3.0 failed 1 times, most recent failure: Lost task 4.0 in stage 3.0 (TID 28) (LAPTOP-II9OQKGK executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\anaconda\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 3.0 failed 1 times, most recent failure: Lost task 4.0 in stage 3.0 (TID 28) (LAPTOP-II9OQKGK executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.write.mode(\"overwrite\").parquet(\"mnist_784\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
